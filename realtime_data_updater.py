#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ
================================================
API ë°ì´í„°ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ê°±ì‹ í•˜ì—¬ í•­ìƒ ìµœì‹  ì •ë³´ë¥¼ ì œê³µ

í•µì‹¬ ê¸°ëŠ¥:
1. ì„œìš¸ì‹œ APIì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
2. ì£¼ê¸°ì  ìë™ ì—…ë°ì´íŠ¸
3. ë°ì´í„° ë²„ì „ ê´€ë¦¬
4. ë³€ê²½ ì‚¬í•­ ì¶”ì  ë° ì•Œë¦¼
"""

import json
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import time
import threading
import logging
from collections import defaultdict


class RealtimeDataUpdater:
    """ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        data_dir: str = "outputs/realtime_data",
        update_interval_hours: int = 6
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            api_key: ì„œìš¸ì‹œ API í‚¤ (ì„ íƒ)
            data_dir: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            update_interval_hours: ì—…ë°ì´íŠ¸ ê°„ê²© (ì‹œê°„)
        """
        self.api_key = api_key
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True, parents=True)
        
        self.update_interval_hours = update_interval_hours
        
        # ë¡œê¹… ì„¤ì •
        self._setup_logging()
        
        # ë°ì´í„° ë²„ì „ ê´€ë¦¬
        self.version_file = self.data_dir / "version.json"
        self.current_version = self._load_version()
        
        # API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
        self.api_endpoints = self._define_api_endpoints()
    
    def _setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_file = self.data_dir / "update.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def _define_api_endpoints(self) -> Dict:
        """
        API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
        
        Returns:
            API ì—”ë“œí¬ì¸íŠ¸ ë”•ì…”ë„ˆë¦¬
        """
        return {
            'commercial_area': {
                'name': 'ì„œìš¸ì‹œ ìš°ë¦¬ë§ˆì„ê°€ê²Œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤',
                'url': 'http://openapi.seoul.go.kr:8088/{api_key}/json/VwsmSignguStorW/',
                'description': 'ìƒê¶Œ í™œì„±í™” ì§€ìˆ˜ ë° ì—…ì¢… ì •ë³´',
                'update_frequency': 'daily'
            },
            'population': {
                'name': 'ì„œìš¸ì‹œ ìš°ë¦¬ë§ˆì„ê°€ê²Œ ìƒê¶Œë¶„ì„ì„œë¹„ìŠ¤(ì—…ì¢…ë³„ ìƒì£¼ì¸êµ¬)',
                'url': 'http://openapi.seoul.go.kr:8088/{api_key}/json/VwsmSignguStorW2/',
                'description': 'ì—…ì¢…ë³„ ìƒì£¼ì¸êµ¬ ì •ë³´',
                'update_frequency': 'weekly'
            },
            'card_usage': {
                'name': 'ì„œìš¸ì‹œ ë¹…ë°ì´í„°ìº í¼ìŠ¤ ì¹´ë“œ ì‚¬ìš© ë°ì´í„°',
                'description': 'ì¹´ë“œ ì‚¬ìš© íŠ¸ë Œë“œ ë¶„ì„',
                'update_frequency': 'monthly',
                'simulated': True  # ì‹¤ì œ API ì—†ìŒ, ì‹œë®¬ë ˆì´ì…˜
            }
        }
    
    def _load_version(self) -> Dict:
        """í˜„ì¬ ë°ì´í„° ë²„ì „ ë¡œë“œ"""
        if self.version_file.exists():
            with open(self.version_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            return {
                'version': '0.0.0',
                'last_update': None,
                'datasets': {}
            }
    
    def _save_version(self):
        """ë°ì´í„° ë²„ì „ ì €ì¥"""
        with open(self.version_file, 'w', encoding='utf-8') as f:
            json.dump(self.current_version, f, ensure_ascii=False, indent=2)
    
    def fetch_commercial_area_data(self) -> Optional[pd.DataFrame]:
        """
        ìƒê¶Œ ë°ì´í„° ìˆ˜ì§‘
        
        Returns:
            ìƒê¶Œ ë°ì´í„° DataFrame
        """
        self.logger.info("ìƒê¶Œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        # ì‹¤ì œ API í˜¸ì¶œ (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
        if self.api_key:
            try:
                url = self.api_endpoints['commercial_area']['url'].format(api_key=self.api_key)
                url += '1/1000/'  # í˜ì´ì§•
                
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                # ë°ì´í„° íŒŒì‹±
                if 'VwsmSignguStorW' in data and 'row' in data['VwsmSignguStorW']:
                    df = pd.DataFrame(data['VwsmSignguStorW']['row'])
                    self.logger.info(f"  âœ“ {len(df)}ê°œ ìƒê¶Œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                    return df
                else:
                    self.logger.warning("  âš ï¸  API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    return None
                    
            except Exception as e:
                self.logger.error(f"  âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
                return None
        
        # API í‚¤ê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        else:
            self.logger.info("  âš™ï¸  ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì¤‘...")
            return self._generate_simulated_commercial_data()
    
    def _generate_simulated_commercial_data(self) -> pd.DataFrame:
        """ì‹œë®¬ë ˆì´ì…˜ ìƒê¶Œ ë°ì´í„° ìƒì„±"""
        
        regions = [
            'ê°•ë‚¨êµ¬', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ê°•ì„œêµ¬', 'ê´€ì•…êµ¬', 'ê´‘ì§„êµ¬',
            'êµ¬ë¡œêµ¬', 'ê¸ˆì²œêµ¬', 'ë…¸ì›êµ¬', 'ë„ë´‰êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬',
            'ë§ˆí¬êµ¬', 'ì„œëŒ€ë¬¸êµ¬', 'ì„œì´ˆêµ¬', 'ì„±ë™êµ¬', 'ì„±ë¶êµ¬', 'ì†¡íŒŒêµ¬',
            'ì–‘ì²œêµ¬', 'ì˜ë“±í¬êµ¬', 'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ì¤‘ë‘êµ¬'
        ]
        
        # ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ê°’ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
        current_hour = datetime.now().hour
        time_factor = 1.0 + 0.1 * np.sin(current_hour * np.pi / 12)  # ì‹œê°„ëŒ€ë³„ ë³€ë™
        
        data = []
        for region in regions:
            base_activity = np.random.uniform(60, 95)
            base_specialization = np.random.uniform(50, 90)
            
            data.append({
                'region': region,
                'commercial_activity': base_activity * time_factor,
                'specialization_score': base_specialization,
                'population': np.random.randint(200000, 600000),
                'store_count': np.random.randint(5000, 15000),
                'timestamp': datetime.now().isoformat()
            })
        
        return pd.DataFrame(data)
    
    def update_all_datasets(self) -> Dict:
        """
        ëª¨ë“  ë°ì´í„°ì…‹ ì—…ë°ì´íŠ¸
        
        Returns:
            ì—…ë°ì´íŠ¸ ê²°ê³¼
        """
        self.logger.info("=" * 80)
        self.logger.info("ì „ì²´ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘")
        self.logger.info("=" * 80)
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'success': [],
            'failed': [],
            'skipped': []
        }
        
        # ìƒê¶Œ ë°ì´í„° ì—…ë°ì´íŠ¸
        try:
            commercial_df = self.fetch_commercial_area_data()
            
            if commercial_df is not None:
                # ë°ì´í„° ì €ì¥
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                file_path = self.data_dir / f"commercial_area_{timestamp}.csv"
                commercial_df.to_csv(file_path, index=False, encoding='utf-8-sig')
                
                # ìµœì‹  ë²„ì „ìœ¼ë¡œ ì‹¬ë³¼ë¦­ ë§í¬ (ë˜ëŠ” ë³µì‚¬)
                latest_path = self.data_dir / "commercial_area_latest.csv"
                commercial_df.to_csv(latest_path, index=False, encoding='utf-8-sig')
                
                # ë²„ì „ ì •ë³´ ì—…ë°ì´íŠ¸
                self.current_version['datasets']['commercial_area'] = {
                    'last_update': timestamp,
                    'file': str(file_path),
                    'records': len(commercial_df)
                }
                
                results['success'].append('commercial_area')
                self.logger.info(f"âœ… ìƒê¶Œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(commercial_df)}ê°œ ë ˆì½”ë“œ")
            else:
                results['failed'].append('commercial_area')
                self.logger.warning("âš ï¸  ìƒê¶Œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                
        except Exception as e:
            results['failed'].append('commercial_area')
            self.logger.error(f"âŒ ìƒê¶Œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
        
        # ê¸°íƒ€ ë°ì´í„°ì…‹ë„ ìœ ì‚¬í•˜ê²Œ ì²˜ë¦¬...
        
        # ë²„ì „ ì •ë³´ ì €ì¥
        self.current_version['last_update'] = results['timestamp']
        self.current_version['version'] = self._increment_version(self.current_version['version'])
        self._save_version()
        
        self.logger.info("=" * 80)
        self.logger.info(f"ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì„±ê³µ {len(results['success'])}, ì‹¤íŒ¨ {len(results['failed'])}")
        self.logger.info(f"ìƒˆ ë²„ì „: {self.current_version['version']}")
        self.logger.info("=" * 80)
        
        return results
    
    def _increment_version(self, version: str) -> str:
        """ë²„ì „ ë²ˆí˜¸ ì¦ê°€"""
        parts = version.split('.')
        parts[-1] = str(int(parts[-1]) + 1)
        return '.'.join(parts)
    
    def get_latest_data(self, dataset_name: str = 'commercial_area') -> Optional[pd.DataFrame]:
        """
        ìµœì‹  ë°ì´í„° ë¡œë“œ
        
        Args:
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            ìµœì‹  ë°ì´í„° DataFrame
        """
        latest_path = self.data_dir / f"{dataset_name}_latest.csv"
        
        if latest_path.exists():
            return pd.read_csv(latest_path, encoding='utf-8-sig')
        else:
            self.logger.warning(f"ìµœì‹  ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {latest_path}")
            return None
    
    def get_data_freshness(self) -> Dict:
        """
        ë°ì´í„° ì‹ ì„ ë„ í™•ì¸
        
        Returns:
            ë°ì´í„°ì…‹ë³„ ì‹ ì„ ë„ ì •ë³´
        """
        freshness = {}
        
        for dataset_name, dataset_info in self.current_version['datasets'].items():
            last_update = datetime.fromisoformat(dataset_info['last_update'])
            age_hours = (datetime.now() - last_update).total_seconds() / 3600
            
            if age_hours < 1:
                status = 'very_fresh'
                description = 'ë§¤ìš° ì‹ ì„ '
            elif age_hours < 6:
                status = 'fresh'
                description = 'ì‹ ì„ '
            elif age_hours < 24:
                status = 'acceptable'
                description = 'ë³´í†µ'
            else:
                status = 'stale'
                description = 'ì—…ë°ì´íŠ¸ í•„ìš”'
            
            freshness[dataset_name] = {
                'last_update': dataset_info['last_update'],
                'age_hours': age_hours,
                'status': status,
                'description': description
            }
        
        return freshness
    
    def compare_data_versions(
        self,
        old_version: str,
        new_version: str
    ) -> Dict:
        """
        ë°ì´í„° ë²„ì „ ë¹„êµ
        
        Args:
            old_version: ì´ì „ ë²„ì „ íƒ€ì„ìŠ¤íƒ¬í”„
            new_version: ìƒˆ ë²„ì „ íƒ€ì„ìŠ¤íƒ¬í”„
            
        Returns:
            ë³€ê²½ ì‚¬í•­ ë”•ì…”ë„ˆë¦¬
        """
        old_file = self.data_dir / f"commercial_area_{old_version}.csv"
        new_file = self.data_dir / f"commercial_area_{new_version}.csv"
        
        if not old_file.exists() or not new_file.exists():
            return {'error': 'íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
        
        old_df = pd.read_csv(old_file, encoding='utf-8-sig')
        new_df = pd.read_csv(new_file, encoding='utf-8-sig')
        
        changes = {
            'added_regions': [],
            'removed_regions': [],
            'significant_changes': []
        }
        
        # ì¶”ê°€ëœ ì§€ì—­
        if 'region' in old_df.columns and 'region' in new_df.columns:
            old_regions = set(old_df['region'].unique())
            new_regions = set(new_df['region'].unique())
            
            changes['added_regions'] = list(new_regions - old_regions)
            changes['removed_regions'] = list(old_regions - new_regions)
        
        # ì£¼ìš” ë³€ê²½ ì‚¬í•­ (í™œì„±ë„ 10% ì´ìƒ ë³€í™”)
        if 'commercial_activity' in old_df.columns and 'commercial_activity' in new_df.columns:
            merged = old_df.merge(new_df, on='region', suffixes=('_old', '_new'))
            
            for _, row in merged.iterrows():
                old_val = row.get('commercial_activity_old', 0)
                new_val = row.get('commercial_activity_new', 0)
                
                if old_val > 0:
                    change_pct = abs((new_val - old_val) / old_val * 100)
                    
                    if change_pct > 10:
                        changes['significant_changes'].append({
                            'region': row['region'],
                            'old_value': old_val,
                            'new_value': new_val,
                            'change_percent': change_pct
                        })
        
        return changes
    
    def start_auto_update(self):
        """ìë™ ì—…ë°ì´íŠ¸ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)"""
        
        self.logger.info(f"ìë™ ì—…ë°ì´íŠ¸ ì‹œì‘: {self.update_interval_hours}ì‹œê°„ ê°„ê²©")
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        def run_scheduler():
            while True:
                try:
                    self.update_all_datasets()
                except Exception as e:
                    self.logger.error(f"ìë™ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {str(e)}")
                
                # ë‹¤ìŒ ì—…ë°ì´íŠ¸ê¹Œì§€ ëŒ€ê¸°
                time.sleep(self.update_interval_hours * 3600)
        
        thread = threading.Thread(target=run_scheduler, daemon=True)
        thread.start()
        
        self.logger.info("ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘ ì™„ë£Œ")
    
    def generate_update_report(self) -> str:
        """
        ì—…ë°ì´íŠ¸ ë³´ê³ ì„œ ìƒì„±
        
        Returns:
            ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³´ê³ ì„œ
        """
        freshness = self.get_data_freshness()
        
        report = f"""# ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ë³´ê³ ì„œ

**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**í˜„ì¬ ë²„ì „**: {self.current_version['version']}  
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: {self.current_version['last_update']}

---

## ë°ì´í„° ì‹ ì„ ë„

"""
        
        for dataset_name, info in freshness.items():
            status_emoji = {
                'very_fresh': 'ğŸŸ¢',
                'fresh': 'ğŸŸ¢',
                'acceptable': 'ğŸŸ¡',
                'stale': 'ğŸ”´'
            }.get(info['status'], 'âšª')
            
            report += f"### {status_emoji} {dataset_name}\n"
            report += f"- ìƒíƒœ: {info['description']}\n"
            report += f"- ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {info['last_update']}\n"
            report += f"- ê²½ê³¼ ì‹œê°„: {info['age_hours']:.1f}ì‹œê°„\n\n"
        
        report += "\n---\n\n## ì—…ë°ì´íŠ¸ ì„¤ì •\n\n"
        report += f"- ì—…ë°ì´íŠ¸ ê°„ê²©: {self.update_interval_hours}ì‹œê°„\n"
        report += f"- ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {self.data_dir}\n"
        
        report += "\n---\n\n## API ì—”ë“œí¬ì¸íŠ¸\n\n"
        
        for endpoint_name, endpoint_info in self.api_endpoints.items():
            report += f"### {endpoint_info['name']}\n"
            report += f"- ì„¤ëª…: {endpoint_info['description']}\n"
            report += f"- ì—…ë°ì´íŠ¸ ì£¼ê¸°: {endpoint_info['update_frequency']}\n\n"
        
        return report


def demo_realtime_updater():
    """ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ ë°ëª¨"""
    
    print("=" * 80)
    print("ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ ë°ëª¨")
    print("=" * 80)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (API í‚¤ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ)
    updater = RealtimeDataUpdater(
        api_key=None,  # ì‹¤ì œ ì‚¬ìš© ì‹œ API í‚¤ ì…ë ¥
        data_dir="outputs/realtime_data",
        update_interval_hours=6
    )
    
    print("\n[1ë‹¨ê³„] ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤í–‰...")
    results = updater.update_all_datasets()
    
    print(f"\n  âœ… ì„±ê³µ: {len(results['success'])}ê°œ")
    print(f"  âŒ ì‹¤íŒ¨: {len(results['failed'])}ê°œ")
    print(f"  â­ï¸  ê±´ë„ˆëœ€: {len(results['skipped'])}ê°œ")
    
    # ìµœì‹  ë°ì´í„° ë¡œë“œ
    print("\n[2ë‹¨ê³„] ìµœì‹  ë°ì´í„° í™•ì¸...")
    latest_data = updater.get_latest_data('commercial_area')
    
    if latest_data is not None:
        print(f"\n  ğŸ“Š ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ ì§€ì—­):")
        print(latest_data.head().to_string(index=False))
    
    # ë°ì´í„° ì‹ ì„ ë„ í™•ì¸
    print("\n[3ë‹¨ê³„] ë°ì´í„° ì‹ ì„ ë„ í™•ì¸...")
    freshness = updater.get_data_freshness()
    
    for dataset, info in freshness.items():
        status_emoji = {
            'very_fresh': 'ğŸŸ¢',
            'fresh': 'ğŸŸ¢',
            'acceptable': 'ğŸŸ¡',
            'stale': 'ğŸ”´'
        }.get(info['status'], 'âšª')
        
        print(f"  {status_emoji} {dataset}: {info['description']} ({info['age_hours']:.1f}ì‹œê°„ ê²½ê³¼)")
    
    # ë³´ê³ ì„œ ìƒì„±
    print("\n[4ë‹¨ê³„] ì—…ë°ì´íŠ¸ ë³´ê³ ì„œ ìƒì„±...")
    report = updater.generate_update_report()
    
    report_path = Path("outputs/realtime_update_report.md")
    report_path.write_text(report, encoding='utf-8')
    
    print(f"  âœ“ ë³´ê³ ì„œ ì €ì¥: {report_path}")
    
    print("\n" + "=" * 80)
    print("âœ… ë°ëª¨ ì™„ë£Œ!")
    print("\nğŸ’¡ ìë™ ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•˜ë ¤ë©´:")
    print("   updater.start_auto_update()")
    print("=" * 80)


if __name__ == '__main__':
    demo_realtime_updater()

